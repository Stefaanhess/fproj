{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports,  Settings and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Dataset/network_inputs.npy'\n",
    "label_path = '../Dataset/network_labels.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_npy = np.load(data_path)\n",
    "Y_train_npy = np.load(label_path) * -1\n",
    "X_train_npy = X_train_npy.reshape(X_train_npy.shape[0]//19, 19, X_train_npy.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        self.h1 = nn.Linear(input_dim, 80)\n",
    "        self.h2 = nn.Linear(80, 40)\n",
    "        self.h3 = nn.Linear(40, 20)\n",
    "        self.h4 = nn.Linear(20, 10)\n",
    "        self.h5 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a1 = F.relu(self.h1(X))\n",
    "        a2 = F.relu(self.h2(a1))\n",
    "        a3 = F.relu(self.h3(a2))\n",
    "        a4 = F.relu(self.h4(a3))\n",
    "        out = self.h5(a4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPotential(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeepPotential, self).__init__()\n",
    "        # one subnetwork for every layer:\n",
    "        sub_dim = 18*4\n",
    "        self.sub1 = SubNetwork(sub_dim)\n",
    "        self.sub2 = SubNetwork(sub_dim)\n",
    "        self.sub3 = SubNetwork(sub_dim)\n",
    "        self.sub4 = SubNetwork(sub_dim)\n",
    "        self.sub5 = SubNetwork(sub_dim)\n",
    "        self.sub6 = SubNetwork(sub_dim)\n",
    "        self.sub7 = SubNetwork(sub_dim)\n",
    "        self.sub8 = SubNetwork(sub_dim)\n",
    "        self.sub9 = SubNetwork(sub_dim)\n",
    "        self.sub10 = SubNetwork(sub_dim)\n",
    "        self.sub11 = SubNetwork(sub_dim)\n",
    "        self.sub12 = SubNetwork(sub_dim)\n",
    "        self.sub13 = SubNetwork(sub_dim)\n",
    "        self.sub14 = SubNetwork(sub_dim)\n",
    "        self.sub15 = SubNetwork(sub_dim)\n",
    "        self.sub16 = SubNetwork(sub_dim)\n",
    "        self.sub17 = SubNetwork(sub_dim)\n",
    "        self.sub18 = SubNetwork(sub_dim)\n",
    "        self.sub19 = SubNetwork(sub_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        a1 = F.relu(self.sub1(X[:, 0]))\n",
    "        a2 = F.relu(self.sub2(X[:, 1]))\n",
    "        a3 = F.relu(self.sub3(X[:, 2]))\n",
    "        a4 = F.relu(self.sub4(X[:, 3]))\n",
    "        a5 = F.relu(self.sub1(X[:, 4]))\n",
    "        a6 = F.relu(self.sub2(X[:, 5]))\n",
    "        a7 = F.relu(self.sub3(X[:, 6]))\n",
    "        a8 = F.relu(self.sub4(X[:, 7]))\n",
    "        a9 = F.relu(self.sub1(X[:, 8]))\n",
    "        a10 = F.relu(self.sub2(X[:, 9]))\n",
    "        a11 = F.relu(self.sub3(X[:, 10]))\n",
    "        a12 = F.relu(self.sub4(X[:, 11]))\n",
    "        a13 = F.relu(self.sub1(X[:, 12]))\n",
    "        a14 = F.relu(self.sub2(X[:, 13]))\n",
    "        a15 = F.relu(self.sub3(X[:, 14]))\n",
    "        a16 = F.relu(self.sub4(X[:, 15]))\n",
    "        a17 = F.relu(self.sub1(X[:, 16]))\n",
    "        a18 = F.relu(self.sub2(X[:, 17]))\n",
    "        a19 = F.relu(self.sub3(X[:, 18]))\n",
    "        out = a1 + a2 + a3 + a4 + a5 + a6 \\\n",
    "                + a7 + a8 + a9 + a10 + a11 \\\n",
    "                + a12 + a13 + a14 + a15 + a16 \\\n",
    "                + a17 + a18 + a19\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to learn random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of range for dimension 1 (of size 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00c7eb10027b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepPot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6694cd01ab4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0ma4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0ma5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0ma6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0ma7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fproj/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mIndexSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# else fall through and raise an error in Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fproj/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_shared_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of range for dimension 1 (of size 4)"
     ]
    }
   ],
   "source": [
    "batchsize = 20\n",
    "sub_dim = 18*4\n",
    "atoms = 4\n",
    "X_train = Variable(torch.randn(batchsize, atoms, sub_dim))\n",
    "Y_train = Variable(torch.randn(batchsize, 1), requires_grad=False)\n",
    "\n",
    "deepPot = DeepPotential()\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(deepPot.parameters(), lr=1e-4)\n",
    "\n",
    "for step in range(10000):\n",
    "    Y_pred = deepPot.forward(X_train)\n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "    \n",
    "    print('{}: {}'.format(step, loss.data[0]))\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1.1662346124649048\n",
      "1: 1.1558300256729126\n",
      "2: 1.1722050905227661\n",
      "3: 1.1840635538101196\n",
      "4: 1.1930692195892334\n",
      "5: 1.1924620866775513\n",
      "6: 1.166693925857544\n",
      "7: 1.1775459051132202\n",
      "8: 1.1604896783828735\n",
      "9: 1.1584609746932983\n",
      "10: 1.2018378973007202\n",
      "11: 1.1719539165496826\n",
      "12: 1.18630051612854\n",
      "13: 1.1877150535583496\n",
      "14: 1.1570606231689453\n",
      "15: 1.1662838459014893\n",
      "16: 1.161933183670044\n",
      "17: 1.1994974613189697\n",
      "18: 1.1633514165878296\n",
      "19: 1.1744556427001953\n",
      "20: 1.1534382104873657\n",
      "21: 1.1629655361175537\n",
      "22: 1.1762715578079224\n",
      "23: 1.1637464761734009\n",
      "24: 1.1750767230987549\n",
      "25: 1.1580514907836914\n",
      "26: 1.1878812313079834\n",
      "27: 1.1850559711456299\n",
      "28: 1.1764681339263916\n",
      "29: 1.142721176147461\n",
      "30: 1.1708734035491943\n",
      "31: 1.1746374368667603\n",
      "32: 1.4593689441680908\n",
      "33: 1.4593679904937744\n",
      "34: 1.4593803882598877\n",
      "35: 1.4593762159347534\n",
      "36: 1.459381103515625\n",
      "37: 1.4593788385391235\n",
      "38: 1.4593697786331177\n",
      "39: 1.4593865871429443\n",
      "40: 1.4593836069107056\n",
      "41: 1.459380865097046\n",
      "42: 1.459380865097046\n",
      "43: 1.4593814611434937\n",
      "44: 1.4593703746795654\n",
      "45: 1.4593818187713623\n",
      "46: 1.4593876600265503\n",
      "47: 1.4593840837478638\n",
      "48: 1.459381341934204\n",
      "49: 1.4593806266784668\n",
      "50: 1.4593713283538818\n",
      "51: 1.4593749046325684\n",
      "52: 1.4593784809112549\n",
      "53: 1.4593770503997803\n",
      "54: 1.459376573562622\n",
      "55: 1.4593764543533325\n",
      "56: 1.4593784809112549\n",
      "57: 1.4593756198883057\n",
      "58: 1.4593671560287476\n",
      "59: 1.4593809843063354\n",
      "60: 1.4593803882598877\n",
      "61: 1.4593827724456787\n",
      "62: 1.4593772888183594\n",
      "63: 1.459378719329834\n",
      "64: 1.4593782424926758\n",
      "65: 1.459381341934204\n",
      "66: 1.4593734741210938\n",
      "67: 1.4593778848648071\n",
      "68: 1.4593615531921387\n",
      "69: 1.4593790769577026\n",
      "70: 1.4593760967254639\n",
      "71: 1.4593827724456787\n",
      "72: 1.4593805074691772\n",
      "73: 1.459378957748413\n",
      "74: 1.4593814611434937\n",
      "75: 1.4593790769577026\n",
      "76: 1.4593803882598877\n",
      "77: 1.4593768119812012\n",
      "78: 1.4593803882598877\n",
      "79: 1.4593788385391235\n",
      "80: 1.4593855142593384\n",
      "81: 1.4593822956085205\n",
      "82: 1.4593759775161743\n",
      "83: 1.4593793153762817\n",
      "84: 1.459372878074646\n",
      "85: 1.4593698978424072\n",
      "86: 1.4593790769577026\n",
      "87: 1.4593753814697266\n",
      "88: 1.4593777656555176\n",
      "89: 1.459382176399231\n",
      "90: 1.4593807458877563\n",
      "91: 1.4593836069107056\n",
      "92: 1.4593693017959595\n",
      "93: 1.4593818187713623\n",
      "94: 1.4593780040740967\n",
      "95: 1.4593746662139893\n",
      "96: 1.4593775272369385\n",
      "97: 1.459378957748413\n",
      "98: 1.4593784809112549\n",
      "99: 1.4593772888183594\n",
      "100: 1.4593828916549683\n",
      "101: 1.459374189376831\n",
      "102: 1.459373950958252\n",
      "103: 1.4593766927719116\n",
      "104: 1.4593806266784668\n",
      "105: 1.4593827724456787\n",
      "106: 1.4593781232833862\n",
      "107: 1.4593764543533325\n",
      "108: 1.4593769311904907\n",
      "109: 1.4593790769577026\n",
      "110: 1.4593855142593384\n",
      "111: 1.459373116493225\n",
      "112: 1.4593744277954102\n",
      "113: 1.4593771696090698\n",
      "114: 1.459388017654419\n",
      "115: 1.459370493888855\n",
      "116: 1.4593746662139893\n",
      "117: 1.4593782424926758\n",
      "118: 1.4593735933303833\n",
      "119: 1.4593766927719116\n",
      "120: 1.4593660831451416\n",
      "121: 1.459374189376831\n",
      "122: 1.459370493888855\n",
      "123: 1.4593757390975952\n",
      "124: 1.459380030632019\n",
      "125: 1.4593768119812012\n",
      "126: 1.4593783617019653\n",
      "127: 1.4593777656555176\n",
      "128: 1.4593794345855713\n",
      "129: 1.4593862295150757\n",
      "130: 1.4593724012374878\n",
      "131: 1.4593714475631714\n",
      "132: 1.4593825340270996\n",
      "133: 1.4593783617019653\n",
      "134: 1.459374189376831\n",
      "135: 1.4593722820281982\n",
      "136: 1.459372878074646\n",
      "137: 1.4593863487243652\n",
      "138: 1.4593801498413086\n",
      "139: 1.4593777656555176\n",
      "140: 1.4593784809112549\n",
      "141: 1.4593827724456787\n",
      "142: 1.4593706130981445\n",
      "143: 1.4593777656555176\n",
      "144: 1.459390640258789\n",
      "145: 1.4593719244003296\n",
      "146: 1.459388256072998\n",
      "147: 1.4593709707260132\n",
      "148: 1.4593696594238281\n",
      "149: 1.4593833684921265\n",
      "150: 1.4593842029571533\n",
      "151: 1.4593729972839355\n",
      "152: 1.459375023841858\n",
      "153: 1.4593865871429443\n",
      "154: 1.4593732357025146\n",
      "155: 1.4593753814697266\n",
      "156: 1.4593778848648071\n",
      "157: 1.4593719244003296\n",
      "158: 1.459381341934204\n",
      "159: 1.4593753814697266\n",
      "160: 1.459380865097046\n",
      "161: 1.4593772888183594\n",
      "162: 1.459376335144043\n",
      "163: 1.4593801498413086\n",
      "164: 1.4593746662139893\n",
      "165: 1.4593771696090698\n",
      "166: 1.4593766927719116\n",
      "167: 1.4593760967254639\n",
      "168: 1.4593766927719116\n",
      "169: 1.4593688249588013\n",
      "170: 1.4593747854232788\n",
      "171: 1.4593721628189087\n",
      "172: 1.4593778848648071\n",
      "173: 1.4593816995620728\n",
      "174: 1.4593764543533325\n",
      "175: 1.4593783617019653\n",
      "176: 1.4593766927719116\n",
      "177: 1.4593843221664429\n",
      "178: 1.4593837261199951\n",
      "179: 1.4593812227249146\n",
      "180: 1.4593861103057861\n",
      "181: 1.4593746662139893\n",
      "182: 1.4593737125396729\n",
      "183: 1.4593899250030518\n",
      "184: 1.4593724012374878\n",
      "185: 1.4593760967254639\n",
      "186: 1.4593708515167236\n",
      "187: 1.4593842029571533\n",
      "188: 1.4593760967254639\n",
      "189: 1.459383249282837\n",
      "190: 1.459377408027649\n",
      "191: 1.4593772888183594\n",
      "192: 1.4593738317489624\n",
      "193: 1.4593729972839355\n",
      "194: 1.4593794345855713\n",
      "195: 1.4593759775161743\n",
      "196: 1.4593772888183594\n",
      "197: 1.4593788385391235\n",
      "198: 1.459376335144043\n",
      "199: 1.4593826532363892\n",
      "200: 1.4593839645385742\n",
      "201: 1.4593638181686401\n",
      "202: 1.459380030632019\n",
      "203: 1.459370732307434\n",
      "204: 1.4593842029571533\n",
      "205: 1.4593884944915771\n",
      "206: 1.4593758583068848\n",
      "207: 1.4593760967254639\n",
      "208: 1.459378719329834\n",
      "209: 1.459377646446228\n",
      "210: 1.459378719329834\n",
      "211: 1.4593701362609863\n",
      "212: 1.4593803882598877\n",
      "213: 1.459375023841858\n",
      "214: 1.4593698978424072\n",
      "215: 1.4593867063522339\n",
      "216: 1.4593819379806519\n",
      "217: 1.4593746662139893\n",
      "218: 1.459373950958252\n",
      "219: 1.4593803882598877\n",
      "220: 1.459381103515625\n",
      "221: 1.4593791961669922\n",
      "222: 1.4593751430511475\n",
      "223: 1.45938241481781\n",
      "224: 1.4593870639801025\n",
      "225: 1.4593760967254639\n",
      "226: 1.4593740701675415\n",
      "227: 1.45937979221344\n",
      "228: 1.4593732357025146\n",
      "229: 1.4593843221664429\n",
      "230: 1.4593737125396729\n",
      "231: 1.4593780040740967\n",
      "232: 1.4593814611434937\n",
      "233: 1.4593737125396729\n",
      "234: 1.4593732357025146\n",
      "235: 1.4593770503997803\n",
      "236: 1.459375023841858\n",
      "237: 1.4593632221221924\n",
      "238: 1.4593747854232788\n",
      "239: 1.459377408027649\n",
      "240: 1.459373116493225\n",
      "241: 1.4593660831451416\n",
      "242: 1.4593755006790161\n",
      "243: 1.4593775272369385\n",
      "244: 1.4593751430511475\n",
      "245: 1.4593753814697266\n",
      "246: 1.4593713283538818\n",
      "247: 1.4593758583068848\n",
      "248: 1.4593786001205444\n",
      "249: 1.4593837261199951\n",
      "250: 1.4593768119812012\n",
      "251: 1.4593771696090698\n",
      "252: 1.459380030632019\n",
      "253: 1.4593738317489624\n",
      "254: 1.4593714475631714\n",
      "255: 1.4593737125396729\n",
      "256: 1.4593784809112549\n",
      "257: 1.4593760967254639\n",
      "258: 1.4593660831451416\n",
      "259: 1.4593727588653564\n",
      "260: 1.4593805074691772\n",
      "261: 1.459380865097046\n",
      "262: 1.459368348121643\n",
      "263: 1.4593802690505981\n",
      "264: 1.4593753814697266\n",
      "265: 1.4593803882598877\n",
      "266: 1.459369421005249\n",
      "267: 1.4593784809112549\n",
      "268: 1.4593794345855713\n",
      "269: 1.4593815803527832\n",
      "270: 1.4593855142593384\n",
      "271: 1.4593822956085205\n",
      "272: 1.4593760967254639\n",
      "273: 1.459388017654419\n",
      "274: 1.4593780040740967\n",
      "275: 1.4593809843063354\n",
      "276: 1.4593760967254639\n",
      "277: 1.45938241481781\n",
      "278: 1.459374189376831\n",
      "279: 1.4593749046325684\n",
      "280: 1.459381103515625\n",
      "281: 1.4593799114227295\n",
      "282: 1.4593793153762817\n",
      "283: 1.459373116493225\n",
      "284: 1.459382176399231\n",
      "285: 1.45938241481781\n",
      "286: 1.4593828916549683\n",
      "287: 1.45937979221344\n",
      "288: 1.4593716859817505\n",
      "289: 1.459375262260437\n",
      "290: 1.459378957748413\n",
      "291: 1.4593843221664429\n",
      "292: 1.459381341934204\n",
      "293: 1.4593708515167236\n",
      "294: 1.4593838453292847\n",
      "295: 1.4593760967254639\n",
      "296: 1.4593652486801147\n",
      "297: 1.4593861103057861\n",
      "298: 1.4593706130981445\n",
      "299: 1.4593781232833862\n",
      "300: 1.4593722820281982\n",
      "301: 1.459380865097046\n",
      "302: 1.4593794345855713\n",
      "303: 1.4593844413757324\n",
      "304: 1.4593740701675415\n",
      "305: 1.4593751430511475\n",
      "306: 1.459376335144043\n",
      "307: 1.4593770503997803\n",
      "308: 1.4593794345855713\n",
      "309: 1.4593830108642578\n",
      "310: 1.4593795537948608\n",
      "311: 1.4593671560287476\n",
      "312: 1.4593886137008667\n",
      "313: 1.4593818187713623\n",
      "314: 1.459381341934204\n",
      "315: 1.4593734741210938\n",
      "316: 1.459383487701416\n",
      "317: 1.459384560585022\n",
      "318: 1.45936918258667\n",
      "319: 1.4593809843063354\n",
      "320: 1.459382176399231\n",
      "321: 1.459371566772461\n",
      "322: 1.4593734741210938\n",
      "323: 1.4593756198883057\n",
      "324: 1.4593803882598877\n",
      "325: 1.4593688249588013\n",
      "326: 1.4593777656555176\n",
      "327: 1.4593666791915894\n",
      "328: 1.4593796730041504\n",
      "329: 1.459377646446228\n",
      "330: 1.4593791961669922\n",
      "331: 1.4593894481658936\n",
      "332: 1.459376573562622\n",
      "333: 1.4593684673309326\n",
      "334: 1.4593757390975952\n",
      "335: 1.459375262260437\n",
      "336: 1.4593746662139893\n",
      "337: 1.4593796730041504\n",
      "338: 1.459372878074646\n",
      "339: 1.4593756198883057\n",
      "340: 1.459376573562622\n",
      "341: 1.4593762159347534\n",
      "342: 1.4593732357025146\n",
      "343: 1.459387183189392\n",
      "344: 1.4593826532363892\n",
      "345: 1.45937180519104\n",
      "346: 1.4593889713287354\n",
      "347: 1.459385633468628\n",
      "348: 1.4593796730041504\n",
      "349: 1.4593870639801025\n",
      "350: 1.4593855142593384\n",
      "351: 1.4593861103057861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352: 1.4593753814697266\n",
      "353: 1.4593888521194458\n",
      "354: 1.4593756198883057\n",
      "355: 1.4593747854232788\n",
      "356: 1.4593753814697266\n",
      "357: 1.4593740701675415\n",
      "358: 1.4593721628189087\n",
      "359: 1.4593662023544312\n",
      "360: 1.4593842029571533\n",
      "361: 1.4593727588653564\n",
      "362: 1.4593788385391235\n",
      "363: 1.4593784809112549\n",
      "364: 1.459377408027649\n",
      "365: 1.4593698978424072\n",
      "366: 1.459376573562622\n",
      "367: 1.4593745470046997\n",
      "368: 1.4593772888183594\n",
      "369: 1.4593737125396729\n",
      "370: 1.4593846797943115\n",
      "371: 1.4593775272369385\n",
      "372: 1.4593775272369385\n",
      "373: 1.4593843221664429\n",
      "374: 1.4593799114227295\n",
      "375: 1.459376573562622\n",
      "376: 1.4593709707260132\n",
      "377: 1.4593799114227295\n",
      "378: 1.4593769311904907\n",
      "379: 1.4593673944473267\n",
      "380: 1.459376573562622\n",
      "381: 1.4593802690505981\n",
      "382: 1.4593857526779175\n",
      "383: 1.4593830108642578\n",
      "384: 1.4593851566314697\n",
      "385: 1.4593749046325684\n",
      "386: 1.4593815803527832\n",
      "387: 1.4593632221221924\n",
      "388: 1.4593781232833862\n",
      "389: 1.4593831300735474\n",
      "390: 1.4593846797943115\n",
      "391: 1.4593722820281982\n",
      "392: 1.4593735933303833\n",
      "393: 1.4593818187713623\n",
      "394: 1.4593733549118042\n",
      "395: 1.4593780040740967\n",
      "396: 1.4593738317489624\n",
      "397: 1.4593737125396729\n",
      "398: 1.4593802690505981\n",
      "399: 1.4593794345855713\n",
      "400: 1.459380865097046\n",
      "401: 1.4593822956085205\n",
      "402: 1.4593814611434937\n",
      "403: 1.4593788385391235\n",
      "404: 1.4593660831451416\n",
      "405: 1.4593784809112549\n",
      "406: 1.4593746662139893\n",
      "407: 1.4593698978424072\n",
      "408: 1.4593827724456787\n",
      "409: 1.4593755006790161\n",
      "410: 1.4593827724456787\n",
      "411: 1.459378719329834\n",
      "412: 1.4593738317489624\n",
      "413: 1.4593782424926758\n",
      "414: 1.4593740701675415\n",
      "415: 1.4593770503997803\n",
      "416: 1.4593758583068848\n",
      "417: 1.4593755006790161\n",
      "418: 1.4593756198883057\n",
      "419: 1.459369421005249\n",
      "420: 1.4593784809112549\n",
      "421: 1.459372639656067\n",
      "422: 1.4593820571899414\n",
      "423: 1.4593770503997803\n",
      "424: 1.4593805074691772\n",
      "425: 1.4593766927719116\n",
      "426: 1.4593695402145386\n",
      "427: 1.4593751430511475\n",
      "428: 1.459380865097046\n",
      "429: 1.4593719244003296\n",
      "430: 1.4593760967254639\n",
      "431: 1.4593775272369385\n",
      "432: 1.4593788385391235\n",
      "433: 1.4593818187713623\n",
      "434: 1.4593772888183594\n",
      "435: 1.4593775272369385\n",
      "436: 1.45938241481781\n",
      "437: 1.459369421005249\n",
      "438: 1.459377646446228\n",
      "439: 1.4593782424926758\n",
      "440: 1.459376573562622\n",
      "441: 1.4593777656555176\n",
      "442: 1.4593725204467773\n",
      "443: 1.4593713283538818\n",
      "444: 1.459381341934204\n",
      "445: 1.4593822956085205\n",
      "446: 1.4593802690505981\n",
      "447: 1.4593822956085205\n",
      "448: 1.4593909978866577\n",
      "449: 1.45937979221344\n",
      "450: 1.4593780040740967\n",
      "451: 1.4593837261199951\n",
      "452: 1.4593727588653564\n",
      "453: 1.4593777656555176\n",
      "454: 1.4593828916549683\n",
      "455: 1.459378719329834\n",
      "456: 1.4593799114227295\n",
      "457: 1.4593867063522339\n",
      "458: 1.459370732307434\n",
      "459: 1.45938241481781\n",
      "460: 1.4593791961669922\n",
      "461: 1.4593870639801025\n",
      "462: 1.4593859910964966\n",
      "463: 1.4593791961669922\n",
      "464: 1.459380865097046\n",
      "465: 1.4593749046325684\n",
      "466: 1.4593807458877563\n",
      "467: 1.4593807458877563\n",
      "468: 1.4593836069107056\n",
      "469: 1.4593770503997803\n",
      "470: 1.459368109703064\n",
      "471: 1.4593755006790161\n",
      "472: 1.459381341934204\n",
      "473: 1.45937180519104\n",
      "474: 1.4593708515167236\n",
      "475: 1.459377408027649\n",
      "476: 1.4593689441680908\n",
      "477: 1.4593733549118042\n",
      "478: 1.459385871887207\n",
      "479: 1.4593762159347534\n",
      "480: 1.4593818187713623\n",
      "481: 1.459378719329834\n",
      "482: 1.459385633468628\n",
      "483: 1.4593744277954102\n",
      "484: 1.4593855142593384\n",
      "485: 1.4593876600265503\n",
      "486: 1.4593716859817505\n",
      "487: 1.4593815803527832\n",
      "488: 1.4593753814697266\n",
      "489: 1.4593781232833862\n",
      "490: 1.4593875408172607\n",
      "491: 1.459368109703064\n",
      "492: 1.4593708515167236\n",
      "493: 1.4593799114227295\n",
      "494: 1.459370493888855\n",
      "495: 1.4593734741210938\n",
      "496: 1.459373950958252\n",
      "497: 1.4593775272369385\n",
      "498: 1.4593732357025146\n",
      "499: 1.459368109703064\n",
      "500: 1.459381341934204\n",
      "501: 1.4593756198883057\n",
      "502: 1.4593827724456787\n",
      "503: 1.4593770503997803\n",
      "504: 1.459378957748413\n",
      "505: 1.4593654870986938\n",
      "506: 1.4593794345855713\n",
      "507: 1.4593799114227295\n",
      "508: 1.4593743085861206\n",
      "509: 1.4593757390975952\n",
      "510: 1.4593844413757324\n",
      "511: 1.4593756198883057\n",
      "512: 1.4593770503997803\n",
      "513: 1.4593772888183594\n",
      "514: 1.4593807458877563\n",
      "515: 1.4593720436096191\n",
      "516: 1.4593828916549683\n",
      "517: 1.4593788385391235\n",
      "518: 1.4593727588653564\n",
      "519: 1.4593849182128906\n",
      "520: 1.4593757390975952\n",
      "521: 1.4593703746795654\n",
      "522: 1.4593799114227295\n",
      "523: 1.4593766927719116\n",
      "524: 1.4593807458877563\n",
      "525: 1.4593830108642578\n",
      "526: 1.4593807458877563\n",
      "527: 1.4593768119812012\n",
      "528: 1.4593788385391235\n",
      "529: 1.4593679904937744\n",
      "530: 1.4593756198883057\n",
      "531: 1.459381341934204\n",
      "532: 1.4593676328659058\n",
      "533: 1.4593749046325684\n",
      "534: 1.4593781232833862\n",
      "535: 1.4593732357025146\n",
      "536: 1.459381341934204\n",
      "537: 1.459372639656067\n",
      "538: 1.459378957748413\n",
      "539: 1.4593795537948608\n",
      "540: 1.4593753814697266\n",
      "541: 1.4593753814697266\n",
      "542: 1.4593662023544312\n",
      "543: 1.4593708515167236\n",
      "544: 1.4593745470046997\n",
      "545: 1.4593751430511475\n",
      "546: 1.4593781232833862\n",
      "547: 1.4593818187713623\n",
      "548: 1.4593778848648071\n",
      "549: 1.4593839645385742\n",
      "550: 1.4593676328659058\n",
      "551: 1.4593727588653564\n",
      "552: 1.459378957748413\n",
      "553: 1.4593790769577026\n",
      "554: 1.4593816995620728\n",
      "555: 1.4593720436096191\n",
      "556: 1.4593768119812012\n",
      "557: 1.4593780040740967\n",
      "558: 1.4593819379806519\n",
      "559: 1.4593806266784668\n",
      "560: 1.4593806266784668\n",
      "561: 1.4593820571899414\n",
      "562: 1.4593756198883057\n",
      "563: 1.459377646446228\n",
      "564: 1.459380030632019\n",
      "565: 1.4593864679336548\n",
      "566: 1.4593734741210938\n",
      "567: 1.459373950958252\n",
      "568: 1.459388017654419\n",
      "569: 1.4593720436096191\n",
      "570: 1.4593855142593384\n",
      "571: 1.4593775272369385\n",
      "572: 1.4593775272369385\n",
      "573: 1.4593771696090698\n",
      "574: 1.4593756198883057\n",
      "575: 1.4593793153762817\n",
      "576: 1.4593840837478638\n",
      "577: 1.4593799114227295\n",
      "578: 1.4593749046325684\n",
      "579: 1.4593806266784668\n",
      "580: 1.4593805074691772\n",
      "581: 1.459381103515625\n",
      "582: 1.4593807458877563\n",
      "583: 1.4593799114227295\n",
      "584: 1.4593868255615234\n",
      "585: 1.4593780040740967\n",
      "586: 1.459378719329834\n",
      "587: 1.4593758583068848\n",
      "588: 1.459381103515625\n",
      "589: 1.4593764543533325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a1b1d8ca1038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fproj/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fproj/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batchsize = 20\n",
    "\n",
    "X_train = Variable(torch.Tensor(X_train_npy))\n",
    "Y_train = Variable(torch.Tensor(Y_train_npy), requires_grad=False)\n",
    "\n",
    "deepPot = DeepPotential()\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(deepPot.parameters(), lr=1e-10)\n",
    "\n",
    "for step in range(10000):\n",
    "    ids = np.random.randint(0, X_train_npy.shape[0], 20).tolist()\n",
    "    Y_pred = deepPot.forward(X_train[ids])\n",
    "    loss = loss_fn(Y_pred, Y_train[ids])\n",
    "    \n",
    "    print('{}: {}'.format(step, loss.data[0]))\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_npy/=1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(X_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fproj)",
   "language": "python",
   "name": "fproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
